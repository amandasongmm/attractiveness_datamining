{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "landMarks = pd.read_csv('../../processing/landmarking/mit/mitLandmarks.csv',index_col = 0)\n",
    "images = list(set(landMarks.index.tolist()))\n",
    "images.sort()\n",
    "X_LMmatrix = np.zeros((len(images),68))\n",
    "Y_LMmatrix = np.zeros((len(images),68))\n",
    "\n",
    "# x = landMarks.ix[images[0]].x.tolist()\n",
    "# print x\n",
    "imList = images\n",
    "for im in images:\n",
    "    ind_im = images.index(im)\n",
    "    imList[ind_im] = im[3:]\n",
    "    X_LMmatrix[ind_im,:] = landMarks.ix[im].x.tolist()\n",
    "    Y_LMmatrix[ind_im,:] = landMarks.ix[im].y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import colorsys\n",
    "import random\n",
    "# print Rectangle((someX, someY), yLength, yLength*0.5)\n",
    "def get3RegionRect(X_LMmatrix,Y_LMmatrix, imInd):\n",
    "    yLengthArr = []\n",
    "    xLengthArr = []\n",
    "    someX_Arr = []\n",
    "    someY_Arr = []\n",
    "    #left patch\n",
    "    yLengthArr.append(Y_LMmatrix[imInd,4]-Y_LMmatrix[imInd,3])\n",
    "    xLengthArr.append((X_LMmatrix[imInd,48]-X_LMmatrix[imInd,2])/2)\n",
    "    someX_Arr.append((X_LMmatrix[imInd,48]+X_LMmatrix[imInd,3])/2-xLengthArr[0]/2) \n",
    "    someY_Arr.append((Y_LMmatrix[imInd,2]+Y_LMmatrix[imInd,29])/2-yLengthArr[0]/2)\n",
    "    #right patch\n",
    "    yLengthArr.append(Y_LMmatrix[imInd,12]-Y_LMmatrix[imInd,13])\n",
    "    xLengthArr.append((X_LMmatrix[imInd,12]-X_LMmatrix[imInd,54])/2)\n",
    "    someX_Arr.append((X_LMmatrix[imInd,54]+X_LMmatrix[imInd,13])/2-xLengthArr[1]/2) \n",
    "    someY_Arr.append((Y_LMmatrix[imInd,29]+Y_LMmatrix[imInd,14])/2-yLengthArr[1]/2)\n",
    "    #top patch\n",
    "    yLengthArr.append((X_LMmatrix[imInd,22]-X_LMmatrix[imInd,21])/2)\n",
    "    xLengthArr.append(X_LMmatrix[imInd,22]-X_LMmatrix[imInd,21])\n",
    "    someX_Arr.append((X_LMmatrix[imInd,21]+X_LMmatrix[imInd,22])/2-xLengthArr[2]/2) \n",
    "    someY_Arr.append((Y_LMmatrix[imInd,21]+Y_LMmatrix[imInd,22])/2-yLengthArr[2]/2)\n",
    "    return someX_Arr, someY_Arr, xLengthArr, yLengthArr\n",
    "\n",
    "def findAllCoors(someX, someY, xLength, yLength):\n",
    "    Xmatrix = np.zeros((int(xLength),int(yLength)))\n",
    "    Ymatrix = np.zeros((int(xLength),int(yLength)))\n",
    "    for i in range(int(xLength)):\n",
    "        for j in range(int(yLength)):\n",
    "            Xmatrix[i,j] = int(someX)+i\n",
    "            Ymatrix[i,j] = int(someY)+j\n",
    "    return Xmatrix,Ymatrix\n",
    "\n",
    "def getAveColor (someX, someY, xLength, yLength, imageName):\n",
    "    im = Image.open(imageName)\n",
    "    rgb_im = im.convert('RGB')\n",
    "    Xmatrix,Ymatrix = findAllCoors(someX, someY, xLength, yLength) \n",
    "    Xsize, Ysize = Xmatrix.shape\n",
    "    hValue = np.zeros((Xsize, Ysize))\n",
    "    sValue = np.zeros((Xsize, Ysize))\n",
    "    vValue = np.zeros((Xsize, Ysize))\n",
    "    for i in range(Xsize):\n",
    "        for j in range (Ysize):\n",
    "            r, g, b = rgb_im.getpixel((Xmatrix[i,j],Ymatrix[i,j]))\n",
    "            hValue[i,j],sValue[i,j],vValue[i,j] = colorsys.rgb_to_hsv(r/255.0, g/255.0, b/255.0)\n",
    "    return [hValue.mean(),sValue.mean(),vValue.mean()]\n",
    "\n",
    "rand_int = imList.index(random.choice(imList))\n",
    "meanHSV = np.zeros((len(imList), 3))\n",
    "for imInd in range(len(imList)):\n",
    "    someX_Arr, someY_Arr, xLengthArr, yLengthArr = get3RegionRect(X_LMmatrix,Y_LMmatrix, imInd)\n",
    "    tempMean = np.zeros((1,3))\n",
    "    for j in range(len(someX_Arr)):\n",
    "        tempMean += np.asarray(getAveColor(someX_Arr[j], someY_Arr[j], xLengthArr[j], yLengthArr[j], imList[imInd]))\n",
    "    meanHSV[imInd,:] = tempMean/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H mean:  0.101154218545 , S mean:  0.392194580956 , V mean:  0.767811662084\n",
      "H std:  0.162586053503 , S std:  0.114747951156 , V std:  0.115534603901\n"
     ]
    }
   ],
   "source": [
    "print 'H mean: ', meanHSV[:,0].mean(),', S mean: ', meanHSV[:,1].mean(), ', V mean: ', meanHSV[:,2].mean()\n",
    "print 'H std: ', meanHSV[:,0].std(),', S std: ', meanHSV[:,1].std(),', V std: ', meanHSV[:,2].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothness mean:  36.3988823441 , smoothness std:  26.1724793184\n"
     ]
    }
   ],
   "source": [
    "from skimage import feature, color, io\n",
    "# Compute the Canny filter for two values of sigma\n",
    "def getSmoothness(imageName, someX_Arr, someY_Arr, xLengthArr, yLengthArr, ifPlot = False,sigma=1):\n",
    "    BigImage = color.rgb2gray(io.imread(imageName))\n",
    "    edged = feature.canny(BigImage, sigma = sigma)\n",
    "    #print BigImage.shape\n",
    "    if ifPlot:\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(6, 3), sharex=True, sharey=True)\n",
    "\n",
    "        ax1.imshow(BigImage, cmap=plt.cm.gray)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title('original image', fontsize=20)\n",
    "\n",
    "        ax2.imshow(edged, cmap=plt.cm.gray)\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Canny filter, $\\sigma='+str(sigma)+'$', fontsize=20)\n",
    "\n",
    "        fig.subplots_adjust(wspace=0.02, hspace=0.02, top=0.9,\n",
    "                            bottom=0.02, left=0.02, right=0.98)\n",
    "        plt.show()\n",
    "    aveEdgeNum = 0.0\n",
    "    for j in range(len(someX_Arr)):\n",
    "        image = BigImage[int(someY_Arr[j]):int(someY_Arr[j]+yLengthArr[j])\\\n",
    "                  ,int(someX_Arr[j]):int(someX_Arr[j]+xLengthArr[j])]\n",
    "        edged = feature.canny(image, sigma = sigma)\n",
    "        if ifPlot:\n",
    "            fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(6, 3), sharex=True, sharey=True)\n",
    "\n",
    "            ax1.imshow(image, cmap=plt.cm.gray)\n",
    "            ax1.axis('off')\n",
    "            ax1.set_title('original image', fontsize=20)\n",
    "\n",
    "            ax2.imshow(edged, cmap=plt.cm.gray)\n",
    "            ax2.axis('off')\n",
    "            ax2.set_title('Canny filter, $\\sigma='+str(sigma)+'$', fontsize=20)\n",
    "\n",
    "            fig.subplots_adjust(wspace=0.02, hspace=0.02, top=0.9,\n",
    "                                bottom=0.02, left=0.02, right=0.98)\n",
    "            plt.show()\n",
    "        aveEdgeNum += edged.sum()\n",
    "        #print image.shape\n",
    "    aveEdgeNum /=float(len(someX_Arr))\n",
    "    return aveEdgeNum\n",
    "smoothness = np.zeros((len(imList),1))\n",
    "custom_sigma = 0.33\n",
    "ifPlot = False\n",
    "for imInd in range(len(imList)):\n",
    "    someX_Arr, someY_Arr, xLengthArr, yLengthArr = get3RegionRect(X_LMmatrix,Y_LMmatrix, imInd)\n",
    "    #print someX_Arr, someY_Arr, xLengthArr, yLengthArr\n",
    "    smoothness[imInd] = \\\n",
    "    getSmoothness(imList[imInd], someX_Arr, someY_Arr, xLengthArr, yLengthArr,sigma=custom_sigma,ifPlot = ifPlot)\n",
    "\n",
    "print 'smoothness mean: ',smoothness.mean(),', smoothness std: ', smoothness.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save as a csv\n",
    "from pandas import DataFrame\n",
    "data = np.concatenate((meanHSV,smoothness), axis=1)\n",
    "geometric = pd.read_csv('../../MIT2kFaceDataset/clean_data/geometric_all.csv',index_col = 0)\n",
    "df = DataFrame(data,columns=['h', 's', 'v', 'smoothness'])\n",
    "df = pd.concat([geometric,df], axis=1)\n",
    "df.to_csv('../../MIT2kFaceDataset/clean_data/geometric_wSmoothness.csv',index = False)\n",
    "df = pd.read_csv('../../MIT2kFaceDataset/clean_data/geometric_wSmoothness.csv',index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeDist(X_LMmatrix,Y_LMmatrix, imInd):\n",
    "    pointNum = X_LMmatrix.shape[1]\n",
    "    distArr = np.zeros((1,pointNum*(pointNum-1)/2))\n",
    "    distInd = 0\n",
    "    for i in range(pointNum):\n",
    "        for j in range(i+1,pointNum):\n",
    "            x1 = np.asarray([X_LMmatrix[imInd,i],Y_LMmatrix[imInd,i]])\n",
    "            x2 = np.asarray([X_LMmatrix[imInd,j],Y_LMmatrix[imInd,j]])\n",
    "            distArr[0,distInd] = np.linalg.norm(x1-x2)\n",
    "            distInd +=1\n",
    "    return distArr\n",
    "import sys\n",
    "def computeSlope(X_LMmatrix,Y_LMmatrix, imInd):\n",
    "    pointNum = X_LMmatrix.shape[1]\n",
    "    slopeArr = np.zeros((1,pointNum*(pointNum-1)/2))\n",
    "    slopeInd = 0\n",
    "    for i in range(pointNum):\n",
    "        for j in range(i+1,pointNum):\n",
    "            denominator = float(X_LMmatrix[imInd,i]-X_LMmatrix[imInd,j])\n",
    "            if denominator != 0:\n",
    "                slopeArr[0,slopeInd] = float(Y_LMmatrix[imInd,i]-Y_LMmatrix[imInd,j])/denominator\n",
    "            else:\n",
    "                #print i,j\n",
    "                if float(Y_LMmatrix[imInd,i]-Y_LMmatrix[imInd,j]) > 0:\n",
    "                    slopeArr[0,slopeInd] = sys.float_info.max/100000\n",
    "                elif float(Y_LMmatrix[imInd,i]-Y_LMmatrix[imInd,j]) ==0:\n",
    "                    slopeArr[0,slopeInd] = 0\n",
    "                else:\n",
    "                    slopeArr[0,slopeInd] = sys.float_info.min*100000\n",
    "            slopeInd +=1\n",
    "    return slopeArr\n",
    "\n",
    "\n",
    "distMat = np.zeros((len(imList),X_LMmatrix.shape[1]*(X_LMmatrix.shape[1]-1)/2))\n",
    "slopeMat = np.zeros((len(imList),X_LMmatrix.shape[1]*(X_LMmatrix.shape[1]-1)/2))\n",
    "for imInd in range(len(imList)):\n",
    "    distArr = computeDist(X_LMmatrix,Y_LMmatrix, imInd)\n",
    "    slopeArr = computeSlope(X_LMmatrix,Y_LMmatrix, imInd)\n",
    "    distMat[imInd,:] = distArr\n",
    "    slopeMat[imInd,:] = slopeArr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of PCs needed to retain 2207.000 variance is 2207.\n"
     ]
    }
   ],
   "source": [
    "distSlope = np.concatenate((slopeMat,distMat), axis=1)\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "explained_variance = distSlope.shape[0]\n",
    "sklearn_pca = sklearnPCA(n_components=explained_variance, whiten = True)\n",
    "distSlope_transf = sklearn_pca.fit_transform(distSlope)\n",
    "print 'The number of PCs needed to retain %.3f variance is %d.' \\\n",
    "      % (explained_variance, distSlope_transf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207\n",
      "mean rating:  4.93915489035\n",
      "number of features: 10\n",
      "On validation set:\n",
      "Residual sum of squares: 1.42\n",
      "Variance score is: -0.00\n",
      "Correlation between predicted ratings and actual ratings is: 0.0675\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.39\n",
      "Variance score is: 0.02\n",
      "Correlation between predicted ratings and actual ratings is: 0.1258\n",
      "number of features: 20\n",
      "On validation set:\n",
      "Residual sum of squares: 1.40\n",
      "Variance score is: 0.01\n",
      "Correlation between predicted ratings and actual ratings is: 0.1282\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.35\n",
      "Variance score is: 0.04\n",
      "Correlation between predicted ratings and actual ratings is: 0.2079\n",
      "number of features: 30\n",
      "On validation set:\n",
      "Residual sum of squares: 1.41\n",
      "Variance score is: -0.00\n",
      "Correlation between predicted ratings and actual ratings is: 0.1115\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.34\n",
      "Variance score is: 0.05\n",
      "Correlation between predicted ratings and actual ratings is: 0.2266\n",
      "number of features: 40\n",
      "On validation set:\n",
      "Residual sum of squares: 1.42\n",
      "Variance score is: -0.00\n",
      "Correlation between predicted ratings and actual ratings is: 0.1107\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.33\n",
      "Variance score is: 0.06\n",
      "Correlation between predicted ratings and actual ratings is: 0.2408\n",
      "number of features: 50\n",
      "On validation set:\n",
      "Residual sum of squares: 1.44\n",
      "Variance score is: -0.01\n",
      "Correlation between predicted ratings and actual ratings is: 0.1123\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.30\n",
      "Variance score is: 0.07\n",
      "Correlation between predicted ratings and actual ratings is: 0.2654\n",
      "number of features: 60\n",
      "On validation set:\n",
      "Residual sum of squares: 1.46\n",
      "Variance score is: -0.02\n",
      "Correlation between predicted ratings and actual ratings is: 0.0934\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.30\n",
      "Variance score is: 0.08\n",
      "Correlation between predicted ratings and actual ratings is: 0.2813\n",
      "number of features: 70\n",
      "On validation set:\n",
      "Residual sum of squares: 1.46\n",
      "Variance score is: -0.02\n",
      "Correlation between predicted ratings and actual ratings is: 0.0953\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.29\n",
      "Variance score is: 0.09\n",
      "Correlation between predicted ratings and actual ratings is: 0.2926\n",
      "number of features: 80\n",
      "On validation set:\n",
      "Residual sum of squares: 1.46\n",
      "Variance score is: -0.03\n",
      "Correlation between predicted ratings and actual ratings is: 0.0875\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.29\n",
      "Variance score is: 0.09\n",
      "Correlation between predicted ratings and actual ratings is: 0.2980\n",
      "number of features: 90\n",
      "On validation set:\n",
      "Residual sum of squares: 1.49\n",
      "Variance score is: -0.05\n",
      "Correlation between predicted ratings and actual ratings is: 0.0770\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.27\n",
      "Variance score is: 0.10\n",
      "Correlation between predicted ratings and actual ratings is: 0.3147\n",
      "number of features: 100\n",
      "On validation set:\n",
      "Residual sum of squares: 1.50\n",
      "Variance score is: -0.06\n",
      "Correlation between predicted ratings and actual ratings is: 0.0690\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.27\n",
      "Variance score is: 0.11\n",
      "Correlation between predicted ratings and actual ratings is: 0.3233\n",
      "number of features: 110\n",
      "On validation set:\n",
      "Residual sum of squares: 1.51\n",
      "Variance score is: -0.06\n",
      "Correlation between predicted ratings and actual ratings is: 0.0755\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.24\n",
      "Variance score is: 0.12\n",
      "Correlation between predicted ratings and actual ratings is: 0.3447\n",
      "number of features: 120\n",
      "On validation set:\n",
      "Residual sum of squares: 1.55\n",
      "Variance score is: -0.09\n",
      "Correlation between predicted ratings and actual ratings is: 0.0528\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.23\n",
      "Variance score is: 0.13\n",
      "Correlation between predicted ratings and actual ratings is: 0.3570\n",
      "number of features: 130\n",
      "On validation set:\n",
      "Residual sum of squares: 1.56\n",
      "Variance score is: -0.09\n",
      "Correlation between predicted ratings and actual ratings is: 0.0593\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.22\n",
      "Variance score is: 0.14\n",
      "Correlation between predicted ratings and actual ratings is: 0.3666\n",
      "number of features: 140\n",
      "On validation set:\n",
      "Residual sum of squares: 1.57\n",
      "Variance score is: -0.11\n",
      "Correlation between predicted ratings and actual ratings is: 0.0495\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.21\n",
      "Variance score is: 0.14\n",
      "Correlation between predicted ratings and actual ratings is: 0.3744\n",
      "number of features: 150\n",
      "On validation set:\n",
      "Residual sum of squares: 1.58\n",
      "Variance score is: -0.11\n",
      "Correlation between predicted ratings and actual ratings is: 0.0600\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.19\n",
      "Variance score is: 0.16\n",
      "Correlation between predicted ratings and actual ratings is: 0.3954\n",
      "number of features: 160\n",
      "On validation set:\n",
      "Residual sum of squares: 1.60\n",
      "Variance score is: -0.12\n",
      "Correlation between predicted ratings and actual ratings is: 0.0601\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.17\n",
      "Variance score is: 0.17\n",
      "Correlation between predicted ratings and actual ratings is: 0.4044\n",
      "number of features: 170\n",
      "On validation set:\n",
      "Residual sum of squares: 1.62\n",
      "Variance score is: -0.14\n",
      "Correlation between predicted ratings and actual ratings is: 0.0534\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.17\n",
      "Variance score is: 0.18\n",
      "Correlation between predicted ratings and actual ratings is: 0.4158\n",
      "number of features: 180\n",
      "On validation set:\n",
      "Residual sum of squares: 1.65\n",
      "Variance score is: -0.17\n",
      "Correlation between predicted ratings and actual ratings is: 0.0483\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.15\n",
      "Variance score is: 0.19\n",
      "Correlation between predicted ratings and actual ratings is: 0.4316\n",
      "number of features: 190\n",
      "On validation set:\n",
      "Residual sum of squares: 1.69\n",
      "Variance score is: -0.19\n",
      "Correlation between predicted ratings and actual ratings is: 0.0407\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.14\n",
      "Variance score is: 0.20\n",
      "Correlation between predicted ratings and actual ratings is: 0.4423\n",
      "number of features: 200\n",
      "On validation set:\n",
      "Residual sum of squares: 1.70\n",
      "Variance score is: -0.20\n",
      "Correlation between predicted ratings and actual ratings is: 0.0430\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.13\n",
      "Variance score is: 0.20\n",
      "Correlation between predicted ratings and actual ratings is: 0.4469\n",
      "number of features: 210\n",
      "On validation set:\n",
      "Residual sum of squares: 1.75\n",
      "Variance score is: -0.23\n",
      "Correlation between predicted ratings and actual ratings is: 0.0348\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.11\n",
      "Variance score is: 0.21\n",
      "Correlation between predicted ratings and actual ratings is: 0.4587\n",
      "number of features: 220\n",
      "On validation set:\n",
      "Residual sum of squares: 1.79\n",
      "Variance score is: -0.25\n",
      "Correlation between predicted ratings and actual ratings is: 0.0295\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.09\n",
      "Variance score is: 0.22\n",
      "Correlation between predicted ratings and actual ratings is: 0.4672\n",
      "number of features: 230\n",
      "On validation set:\n",
      "Residual sum of squares: 1.78\n",
      "Variance score is: -0.25\n",
      "Correlation between predicted ratings and actual ratings is: 0.0364\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.09\n",
      "Variance score is: 0.23\n",
      "Correlation between predicted ratings and actual ratings is: 0.4754\n",
      "number of features: 240\n",
      "On validation set:\n",
      "Residual sum of squares: 1.82\n",
      "Variance score is: -0.27\n",
      "Correlation between predicted ratings and actual ratings is: 0.0315\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.07\n",
      "Variance score is: 0.24\n",
      "Correlation between predicted ratings and actual ratings is: 0.4834\n",
      "number of features: 250\n",
      "On validation set:\n",
      "Residual sum of squares: 10.71\n",
      "Variance score is: -6.27\n",
      "Correlation between predicted ratings and actual ratings is: 0.0281\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.07\n",
      "Variance score is: 0.24\n",
      "Correlation between predicted ratings and actual ratings is: 0.4839\n",
      "number of features: 260\n",
      "On validation set:\n",
      "Residual sum of squares: 4.98\n",
      "Variance score is: -2.48\n",
      "Correlation between predicted ratings and actual ratings is: 0.0288\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.07\n",
      "Variance score is: 0.24\n",
      "Correlation between predicted ratings and actual ratings is: 0.4895\n",
      "number of features: 270\n",
      "On validation set:\n",
      "Residual sum of squares: 3.60\n",
      "Variance score is: -1.50\n",
      "Correlation between predicted ratings and actual ratings is: 0.0305\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.05\n",
      "Variance score is: 0.25\n",
      "Correlation between predicted ratings and actual ratings is: 0.4979\n",
      "number of features: 280\n",
      "On validation set:\n",
      "Residual sum of squares: 2.21\n",
      "Variance score is: -0.55\n",
      "Correlation between predicted ratings and actual ratings is: 0.0431\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.05\n",
      "Variance score is: 0.25\n",
      "Correlation between predicted ratings and actual ratings is: 0.4959\n",
      "number of features: 290\n",
      "On validation set:\n",
      "Residual sum of squares: 1.77\n",
      "Variance score is: -0.25\n",
      "Correlation between predicted ratings and actual ratings is: 0.0320\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.06\n",
      "Variance score is: 0.25\n",
      "Correlation between predicted ratings and actual ratings is: 0.4998\n",
      "number of features: 300\n",
      "On validation set:\n",
      "Residual sum of squares: 1.77\n",
      "Variance score is: -0.24\n",
      "Correlation between predicted ratings and actual ratings is: 0.0398\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.04\n",
      "Variance score is: 0.26\n",
      "Correlation between predicted ratings and actual ratings is: 0.5023\n",
      "number of features: 310\n",
      "On validation set:\n",
      "Residual sum of squares: 1.82\n",
      "Variance score is: -0.27\n",
      "Correlation between predicted ratings and actual ratings is: 0.0349\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.03\n",
      "Variance score is: 0.26\n",
      "Correlation between predicted ratings and actual ratings is: 0.5087\n",
      "number of features: 320\n",
      "On validation set:\n",
      "Residual sum of squares: 1.83\n",
      "Variance score is: -0.30\n",
      "Correlation between predicted ratings and actual ratings is: 0.0315\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.03\n",
      "Variance score is: 0.27\n",
      "Correlation between predicted ratings and actual ratings is: 0.5163\n",
      "number of features: 330\n",
      "On validation set:\n",
      "Residual sum of squares: 1.79\n",
      "Variance score is: -0.25\n",
      "Correlation between predicted ratings and actual ratings is: 0.0324\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.01\n",
      "Variance score is: 0.28\n",
      "Correlation between predicted ratings and actual ratings is: 0.5229\n",
      "number of features: 340\n",
      "On validation set:\n",
      "Residual sum of squares: 1.84\n",
      "Variance score is: -0.29\n",
      "Correlation between predicted ratings and actual ratings is: 0.0239\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 1.01\n",
      "Variance score is: 0.29\n",
      "Correlation between predicted ratings and actual ratings is: 0.5319\n",
      "number of features: 350\n",
      "On validation set:\n",
      "Residual sum of squares: 1.84\n",
      "Variance score is: -0.30\n",
      "Correlation between predicted ratings and actual ratings is: 0.0220\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.99\n",
      "Variance score is: 0.30\n",
      "Correlation between predicted ratings and actual ratings is: 0.5427\n",
      "number of features: 360\n",
      "On validation set:\n",
      "Residual sum of squares: 1.87\n",
      "Variance score is: -0.31\n",
      "Correlation between predicted ratings and actual ratings is: 0.0129\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.98\n",
      "Variance score is: 0.30\n",
      "Correlation between predicted ratings and actual ratings is: 0.5468\n",
      "number of features: 370\n",
      "On validation set:\n",
      "Residual sum of squares: 1.90\n",
      "Variance score is: -0.34\n",
      "Correlation between predicted ratings and actual ratings is: 0.0272\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.97\n",
      "Variance score is: 0.31\n",
      "Correlation between predicted ratings and actual ratings is: 0.5532\n",
      "number of features: 380\n",
      "On validation set:\n",
      "Residual sum of squares: 1.90\n",
      "Variance score is: -0.33\n",
      "Correlation between predicted ratings and actual ratings is: 0.0271\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.96\n",
      "Variance score is: 0.32\n",
      "Correlation between predicted ratings and actual ratings is: 0.5595\n",
      "number of features: 390\n",
      "On validation set:\n",
      "Residual sum of squares: 1.88\n",
      "Variance score is: -0.33\n",
      "Correlation between predicted ratings and actual ratings is: 0.0224\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.94\n",
      "Variance score is: 0.33\n",
      "Correlation between predicted ratings and actual ratings is: 0.5700\n",
      "number of features: 400\n",
      "On validation set:\n",
      "Residual sum of squares: 1.91\n",
      "Variance score is: -0.34\n",
      "Correlation between predicted ratings and actual ratings is: 0.0298\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.93\n",
      "Variance score is: 0.34\n",
      "Correlation between predicted ratings and actual ratings is: 0.5797\n",
      "number of features: 410\n",
      "On validation set:\n",
      "Residual sum of squares: 1.92\n",
      "Variance score is: -0.34\n",
      "Correlation between predicted ratings and actual ratings is: 0.0290\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.92\n",
      "Variance score is: 0.34\n",
      "Correlation between predicted ratings and actual ratings is: 0.5816\n",
      "number of features: 420\n",
      "On validation set:\n",
      "Residual sum of squares: 1.94\n",
      "Variance score is: -0.37\n",
      "Correlation between predicted ratings and actual ratings is: 0.0229\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.91\n",
      "Variance score is: 0.35\n",
      "Correlation between predicted ratings and actual ratings is: 0.5873\n",
      "number of features: 430\n",
      "On validation set:\n",
      "Residual sum of squares: 1.97\n",
      "Variance score is: -0.38\n",
      "Correlation between predicted ratings and actual ratings is: 0.0157\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.90\n",
      "Variance score is: 0.36\n",
      "Correlation between predicted ratings and actual ratings is: 0.5956\n",
      "number of features: 440\n",
      "On validation set:\n",
      "Residual sum of squares: 1.98\n",
      "Variance score is: -0.40\n",
      "Correlation between predicted ratings and actual ratings is: 0.0233\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.89\n",
      "Variance score is: 0.36\n",
      "Correlation between predicted ratings and actual ratings is: 0.6001\n",
      "number of features: 450\n",
      "On validation set:\n",
      "Residual sum of squares: 2.04\n",
      "Variance score is: -0.43\n",
      "Correlation between predicted ratings and actual ratings is: 0.0152\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.87\n",
      "Variance score is: 0.38\n",
      "Correlation between predicted ratings and actual ratings is: 0.6108\n",
      "number of features: 460\n",
      "On validation set:\n",
      "Residual sum of squares: 2.06\n",
      "Variance score is: -0.45\n",
      "Correlation between predicted ratings and actual ratings is: 0.0090\n",
      " \n",
      "On training set:\n",
      "Residual sum of squares: 0.87\n",
      "Variance score is: 0.38\n",
      "Correlation between predicted ratings and actual ratings is: 0.6155\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2cc29dba19c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnumFeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumFeatureArr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpredictionModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRidgeCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mvalC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain_Test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_rating\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistSlope_transf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperParam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictionModel\u001b[0m\u001b[0;34m,\u001b[0m                   \u001b[0mnumTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Result/mit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'distSlope'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mvalCorr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Olivialinlin/Documents/Github/attractiveness_datamining/linjieCode/code/xVal_train_test.py\u001b[0m in \u001b[0;36mTrain_Test\u001b[0;34m(mean_rating, featureMat, pModel, hyperParam, xVal, numTrain, savePath, MODEL, printToFile, ratio)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mfeature_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moptNumFea\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Do linear regression on feature_arr and mean_rating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mpModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m######### on validation set #############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Olivialinlin/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                   \u001b[0mgcv_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcv_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                                   store_cv_values=self.store_cv_values)\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_cv_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Olivialinlin/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bad gcv_mode \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgcv_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQT_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0mn_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mcv_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Olivialinlin/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36m_pre_compute_svd\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD not supported for sparse matrices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mUT_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Olivialinlin/anaconda2/lib/python2.7/site-packages/scipy/linalg/decomp_svd.pyc\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD did not converge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         raise ValueError('illegal value in %d-th argument of internal gesdd'\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "mean_rating = geometric.attractive.as_matrix()\n",
    "print len(mean_rating)\n",
    "mean_rating = map(float, mean_rating)\n",
    "mean_rating = np.array(mean_rating)\n",
    "import sys\n",
    "#print sys.path\n",
    "# local\n",
    "PkgPath = '/Users/Olivialinlin/Documents/Github/attractiveness_datamining/linjieCode/code'\n",
    "# server\n",
    "# PkgPath = '/home/lli-ms/attractiveness_datamining/linjieCode/code'\n",
    "\n",
    "if PkgPath not in sys.path:\n",
    "    sys.path.insert(0, PkgPath)\n",
    "#print sys.path\n",
    "from xVal_train_test import Train_Test\n",
    "\n",
    "baseLine = mean_rating.mean()\n",
    "print 'mean rating: ', baseLine\n",
    "import sklearn\n",
    "numFeatureArr = np.linspace(10,2200,num =220 ,dtype = np.int16)\n",
    "valCorr = []\n",
    "trainCorr = []\n",
    "for numFeature in numFeatureArr:\n",
    "    predictionModel = sklearn.linear_model.RidgeCV(alphas=np.logspace(-3,2,num=20), fit_intercept=True)\n",
    "    valC, trainC = Train_Test(mean_rating,distSlope_transf, hyperParam = numFeature, xVal = False, pModel = predictionModel,\\\n",
    "                   numTrain = 50,savePath = '../Result/mit',MODEL= 'distSlope')\n",
    "    \n",
    "    valCorr.append(valC)\n",
    "    trainCorr.append(trainC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.128168131857\n"
     ]
    }
   ],
   "source": [
    "print max(valCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "#imInd = imList.index(random.choice(imList))\n",
    "im = plt.imread(imList[imInd])\n",
    "fSize = 6\n",
    "implot = plt.imshow(im)\n",
    "import pylab\n",
    "# put a blue dot at (10, 20)\n",
    "plt.scatter(X_LMmatrix[imInd,:],Y_LMmatrix[imInd,:])\n",
    "labels = [str(i) for i in range(68)]\n",
    "for label, x, y in zip(labels, X_LMmatrix[imInd,:], Y_LMmatrix[imInd,:]):\n",
    "    plt.annotate(\n",
    "        label, \n",
    "        xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "yLengthArr = []\n",
    "xLengthArr = []\n",
    "someX_Arr = []\n",
    "someY_Arr = []\n",
    "# left patch\n",
    "yLengthArr.append(Y_LMmatrix[imInd,4]-Y_LMmatrix[imInd,3])\n",
    "xLengthArr.append((X_LMmatrix[imInd,48]-X_LMmatrix[imInd,2])/2)\n",
    "someX_Arr.append((X_LMmatrix[imInd,48]+X_LMmatrix[imInd,3])/2-xLength[0]/2) \n",
    "someY_Arr.append((Y_LMmatrix[imInd,2]+Y_LMmatrix[imInd,29])/2-yLength[0]/2)\n",
    "# right patch\n",
    "yLengthArr.append(Y_LMmatrix[imInd,12]-Y_LMmatrix[imInd,13])\n",
    "xLengthArr.append((X_LMmatrix[imInd,12]-X_LMmatrix[imInd,54])/2)\n",
    "someX_Arr.append((X_LMmatrix[imInd,54]+X_LMmatrix[imInd,13])/2-xLength[1]/2) \n",
    "someY_Arr.append((Y_LMmatrix[imInd,29]+Y_LMmatrix[imInd,14])/2-yLength[1]/2)\n",
    "# top patch\n",
    "yLengthArr.append((X_LMmatrix[imInd,22]-X_LMmatrix[imInd,21])/2)\n",
    "xLengthArr.append(X_LMmatrix[imInd,22]-X_LMmatrix[imInd,21])\n",
    "someX_Arr.append((X_LMmatrix[imInd,21]+X_LMmatrix[imInd,22])/2-xLength[2]/2) \n",
    "someY_Arr.append((Y_LMmatrix[imInd,21]+Y_LMmatrix[imInd,22])/2-yLength[2]/2)\n",
    "currentAxis = plt.gca()\n",
    "for i in range(len(xLength)):\n",
    "    currentAxis.add_patch(Rectangle((someX_Arr[i], someY_Arr[i]), xLengthArr[i], yLengthArr[i], facecolor=\"grey\"))\n",
    "    \n",
    "fig = pylab.gcf()\n",
    "fig.set_size_inches(5*fSize, 3*fSize)\n",
    "plt.show()\n",
    "print someX_Arr, someY_Arr, xLengthArr, yLengthArr\n",
    "print X_LMmatrix[imInd,2], Y_LMmatrix[imInd,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im_Image = Image.open(imList[imInd])\n",
    "rgb_im = im_Image.convert('RGB')\n",
    "r, g, b = rgb_im.getpixel((X_LMmatrix[imInd,1],Y_LMmatrix[imInd,1]))\n",
    "print r,g,b\n",
    "import colorsys\n",
    "h,s,v = colorsys.rgb_to_hsv(r/255.0, g/255.0, b/255.0)\n",
    "print h,s,v\n",
    "\n",
    "print im[int(Y_LMmatrix[imInd,1]),int(X_LMmatrix[imInd,1]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "img = color.rgb2gray(io.imread(imList[imInd]))\n",
    "smallRegion = img[int(someX_Arr[1]):int(someX_Arr[1]+xLengthArr[1])\\\n",
    "                  ,int(someY_Arr[1]):int(someY_Arr[1]+yLengthArr[1])]\n",
    "#im = smallRegion\n",
    "im = img\n",
    "custom_sigma = 0.33\n",
    "#edges3 = auto_canny(im, sigma = 0.9)\n",
    "edges1 = feature.canny(im)\n",
    "edges2 = feature.canny(im, sigma = custom_sigma)\n",
    "print edges1.sum()\n",
    "print edges2.sum()\n",
    "#print edges3.sum()\n",
    "# display results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3), sharex=True, sharey=True)\n",
    "\n",
    "ax1.imshow(im, cmap=plt.cm.gray)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('original image', fontsize=20)\n",
    "\n",
    "ax2.imshow(edges1, cmap=plt.cm.gray)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Canny filter, $\\sigma=1$', fontsize=20)\n",
    "\n",
    "ax3.imshow(edges2, cmap=plt.cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Canny filter, $\\sigma='+str(custom_sigma)+'$', fontsize=20)\n",
    "\n",
    "# ax4.imshow(edges3, cmap=plt.cm.gray)\n",
    "# ax4.axis('off')\n",
    "# ax4.set_title('auto', fontsize=20)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.02, hspace=0.02, top=0.9,\n",
    "                    bottom=0.02, left=0.02, right=0.98)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "img = color.rgb2gray(io.imread(imList[imInd]));\n",
    "\n",
    "edge_sobel = sobel(img)\n",
    "edge_scharr = scharr(img)\n",
    "edge_prewitt = prewitt(img)\n",
    "\n",
    "diff_scharr_prewitt = edge_scharr - edge_prewitt\n",
    "diff_scharr_sobel = edge_scharr - edge_sobel\n",
    "max_diff = np.max(np.maximum(diff_scharr_prewitt, diff_scharr_sobel))\n",
    "\n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, subplot_kw={'adjustable':'box-forced'})\n",
    "\n",
    "ax0.imshow(img, cmap=plt.cm.gray)\n",
    "ax0.set_title('Original image')\n",
    "ax0.axis('off')\n",
    "\n",
    "ax1.imshow(edge_scharr, cmap=plt.cm.gray)\n",
    "ax1.set_title('Scharr Edge Detection')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(diff_scharr_prewitt, cmap=plt.cm.jet, vmax=max_diff)\n",
    "ax2.set_title('Scharr - Prewitt')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3.imshow(diff_scharr_sobel, cmap=plt.cm.jet, vmax=max_diff)\n",
    "ax3.set_title('Scharr - Sobel')\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
