{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n",
      "../../MIT2kFaceDataset/2kfaces/\n",
      "mean-subtracted values: [('B', 104.0069879317889), ('G', 116.66876761696767), ('R', 122.6789143406786)]\n",
      "Defining the net!\n",
      "featureNum:  (4096,)\n",
      "2222\n",
      "The number of PCs needed to retain 0.990 variance is 846.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is code for extracting NN features of face image data \n",
    "and then fit a linear model to predict attractiveness of a face\n",
    "Available dataset: TWIN, CHICAGO and MIT\n",
    "Available NN feature: 'caffeNet','vgg16','vggFace' and 'faceSNN'\n",
    "\n",
    "BY Linjie Li\n",
    "Please run this code on guru2 server\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "\n",
    "# Load image dataset#\n",
    "Dataset = 'mit' # 'twin', 'chicago' or 'mit'\n",
    "if Dataset == 'twin':\n",
    "    imPath = '../../processing/imageProcessing/paddedImages/'\n",
    "    ext = '.png'\n",
    "elif Dataset == 'chicago':\n",
    "    imPath = '../../ChicagoFaceDataset/CFD Version 2.0/CFD 2.0 Images/'\n",
    "    ext = 'N.jpg'\n",
    "else:\n",
    "    imPath = '../../MIT2kFaceDataset/2kfaces/'\n",
    "    ext = '.jpg'\n",
    "imList = []\n",
    "for dirpath, dirnames, filenames in os.walk(imPath):\n",
    "    for filename in [f for f in filenames if f.endswith(ext)]:\n",
    "        imList.append(os.path.join(dirpath, filename))\n",
    "imList.sort()\n",
    "print len(imList)\n",
    "print imPath\n",
    "\n",
    "# Make sure that caffe is on the python path:\n",
    "caffe_root = '/home/lli-ms/caffe/'\n",
    "pretrained_model_root = '/home/lli-ms/caffe/'\n",
    "\n",
    "# run this line one time only!\n",
    "import sys\n",
    "caffePython = pretrained_model_root + 'python'\n",
    "if caffePython not in sys.path:\n",
    "    sys.path.insert(0, caffePython)\n",
    "\n",
    "\n",
    "import caffe\n",
    "# Load mean\n",
    "mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "print 'mean-subtracted values:', zip('BGR', mu)\n",
    "\n",
    "# Load the trained net\n",
    "MODEL = 'vggFace' #'caffeNet','vgg16','vggFace' or 'faceSNN'\n",
    "\n",
    "saveFigPath = '../Result/'+Dataset+'/'+MODEL\n",
    "if not os.path.exists(saveFigPath):\n",
    "    os.makedirs(saveFigPath)\n",
    "    \n",
    "if MODEL == 'vgg16':\n",
    "    MODEL_FILE = caffe_root +'models/VGG16/VGG_ILSVRC_16_layers_deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/VGG16/VGG_ILSVRC_16_layers.caffemodel'\n",
    "elif MODEL == 'caffeNet':\n",
    "    MODEL_FILE = caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\n",
    "elif MODEL == 'vggFace':\n",
    "    MODEL_FILE = caffe_root + 'models/VGGFACE/VGG_CNN_F_deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/VGGFACE/VGG_CNN_F.caffemodel'\n",
    "    MEAN_FILE = caffe_root + 'models/VGGFACE/VGG_mean.binaryproto'\n",
    "else:\n",
    "    MODEL = 'faceSNN'\n",
    "    MODEL_FILE = caffe_root +'models/sraonet/siamese_lecun_deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/sraonet/snapshots/sraonet_lecun_gd_sub2_iter_100000.caffemodel'\n",
    "    \n",
    "caffe.set_device(1)\n",
    "caffe.set_mode_gpu()\n",
    "if not os.path.isfile(PRETRAINED_FILE):\n",
    "    print(\"No caffemodel!!!\")\n",
    "elif not os.path.isfile(MODEL_FILE):\n",
    "    print(\"No MODEL !!!\")\n",
    "else:\n",
    "    print \"Defining the net!\"\n",
    "    net = caffe.Net(MODEL_FILE,\n",
    "                PRETRAINED_FILE,\n",
    "                caffe.TEST)\n",
    "# input preprocessing: 'data' is the name of the input blob == net.inputs[0]\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "if MODEL != 'faceSNN':\n",
    "    # subtract the dataset-mean value in each channel\n",
    "    transformer.set_mean('data', mu)\n",
    "    print 'featureNum: ',net.params['fc7'][1].data.shape\n",
    "else:\n",
    "    print 'featureNum: ',net.params['fc6'][1].data.shape\n",
    "transformer.set_transpose('data', (2,0,1))\n",
    "# the reference model operates on images in [0,255] range instead of [0,1]\n",
    "transformer.set_raw_scale('data', 255) \n",
    "# the reference model has channels in BGR order instead of RGB\n",
    "transformer.set_channel_swap('data', (2,1,0))\n",
    "\n",
    "# read in image list \n",
    "def readFile(fName):\n",
    "    text_file = open(fName, \"r\")\n",
    "    lines = text_file.read().split('\\n')\n",
    "    text_file.close()\n",
    "    return lines\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "if MODEL == 'vgg16' or MODEL == 'vggFace':\n",
    "    featureNum = 4096\n",
    "    imgeReshape = [224,224]\n",
    "    featureLayer = 'fc7'\n",
    "elif MODEL == 'caffeNet':\n",
    "    featureNum = 4096\n",
    "    imgeReshape = [227,227]\n",
    "    featureLayer = 'fc7'\n",
    "else:\n",
    "    featureNum = 50\n",
    "    imgeReshape = [56,46]\n",
    "    featureLayer = 'fc6'\n",
    "\n",
    "if Dataset == 'twin':\n",
    "    features = np.zeros([4,len(imList)/4,featureNum])\n",
    "    perImNum = len(imList)/4\n",
    "    img_type_num = {}\n",
    "    img_type_index = {}\n",
    "    img_type_list = {}\n",
    "    type_index = 0\n",
    "else:\n",
    "    features = np.zeros([len(imList),featureNum])\n",
    "totalNum = 0\n",
    "\n",
    "# print len(imList)\n",
    "for img in imList:\n",
    "    imgName = os.path.basename(img)\n",
    "    if imgName.endswith(('.jpg','.png')):\n",
    "        input_image = caffe.io.load_image(img)\n",
    "        net.blobs['data'].reshape(1,3,imgeReshape[0],imgeReshape[1])\n",
    "        net.blobs['data'].data[...] = transformer.preprocess('data', input_image)\n",
    "        out = net.forward()\n",
    "        feat = net.blobs[featureLayer].data\n",
    "        if Dataset =='twin':\n",
    "            img_type = int(imgName[7:-4])/perImNum\n",
    "            img_index = int(imgName[7:-4])%perImNum\n",
    "            #print 'img_type:',img_type\n",
    "            if img_type in img_type_num.keys():\n",
    "                img_type_num[img_type] = img_type_num[img_type] + 1\n",
    "                img_type_list[img_type][img_index] = img\n",
    "            else:\n",
    "                img_type_num[img_type] = 0\n",
    "                img_type_list[img_type] = [None]*perImNum\n",
    "                img_type_index[img_type] = type_index\n",
    "                type_index +=1\n",
    "            #print 'img_type_index:',img_type_index[img_type]\n",
    "            features[img_type_index[img_type],img_type_num[img_type]] = feat.flatten()\n",
    "        else:\n",
    "            # need to be further revised!\n",
    "            features[totalNum] = feat.flatten()\n",
    "            #print features[totalNum]\n",
    "        totalNum +=1\n",
    "    else:\n",
    "        print img\n",
    "#print len(img_type_num)\n",
    "print totalNum\n",
    "#print img_type_list\n",
    "\n",
    "if Dataset == 'twin':\n",
    "    featureMat = np.zeros((totalNum,featureNum))\n",
    "    k = 0\n",
    "    for i in range(features.shape[0]):\n",
    "        for j in range(features[i].shape[0]):\n",
    "            if sum(features[i,j,:])!=0:\n",
    "                featureMat[k,:] = features[i,j,:]\n",
    "                k +=1\n",
    "else:\n",
    "    featureMat = features\n",
    "#print featureMat\n",
    "\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "if MODEL != 'faceSNN':\n",
    "    explained_variance = 0.99\n",
    "    #explained_variance = 50\n",
    "else:\n",
    "    explained_variance = featureNum\n",
    "\n",
    "sklearn_pca = sklearnPCA(n_components=explained_variance, whiten  = True)\n",
    "feature_transf = sklearn_pca.fit_transform(featureMat)\n",
    "print 'The number of PCs needed to retain %.3f variance is %d.' \\\n",
    "      % (explained_variance, feature_transf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lli-ms/attractiveness_datamining/linjieCode/code', '/home/lli-ms/caffe/python', '', '/home/wfedus/projects/deepx', '/usr/local/lib/python2.7/dist-packages/s3cmd-1.6.1-py2.7.egg', '/usr/local/lib/python2.7/dist-packages/python_magic-0.4.11-py2.7.egg', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/usr/lib/pymodules/python2.7', '/usr/lib/python2.7/dist-packages/ubuntu-sso-client', '/usr/local/lib/python2.7/dist-packages/IPython/extensions', '/home/lli-ms/.ipython']\n",
      "mean rating:  4.938305509\n",
      "number of features: 90\n",
      "Residual sum of squares: 0.81\n",
      "Variance score is: 0.42\n",
      "Correlation between predicted ratings and actual ratings is: 0.6430\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#print sys.path\n",
    "# local\n",
    "# PkgPath = '/Users/Olivialinlin/Documents/Github/attractiveness_datamining/linjieCode/code'\n",
    "# server\n",
    "PkgPath = '/home/lli-ms/attractiveness_datamining/linjieCode/code'\n",
    "\n",
    "if PkgPath not in sys.path:\n",
    "    sys.path.insert(0, PkgPath)\n",
    "print sys.path\n",
    "from xVal_train_test import Train_Test\n",
    "import pandas as pd\n",
    "ratingPath = '../Result/'+Dataset+'/meanRating.csv'\n",
    "mean_rating = pd.read_csv(ratingPath,index_col = 0).as_matrix()[:,0].tolist()\n",
    "mean_rating = map(float, mean_rating)\n",
    "mean_rating = np.array(mean_rating)\n",
    "\n",
    "baseLine = mean_rating.mean()\n",
    "print 'mean rating: ', baseLine\n",
    "import sklearn\n",
    "predictionModel = sklearn.linear_model.RidgeCV(alphas=np.logspace(-3,2,num=20), fit_intercept=True)\n",
    "Train_Test(mean_rating, feature_transf,xVal = True, pModel = predictionModel,\\\n",
    "               numTrain = 50,savePath = '../Result/'+Dataset,MODEL= MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207\n",
      "../../MIT2kFaceDataset/2kfaces/\n",
      "mean-subtracted values: [('B', 104.0069879317889), ('G', 116.66876761696767), ('R', 122.6789143406786)]\n",
      "Defining the net!\n",
      "featureNum:  (4096,)\n",
      "2207\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "# Load image dataset#\n",
    "Dataset = 'mit' # 'twin', 'chicago' or 'mit'\n",
    "if Dataset == 'twin':\n",
    "    imPath = '../../processing/imageProcessing/paddedImages/'\n",
    "    ext = '.png'\n",
    "elif Dataset == 'chicago':\n",
    "    imPath = '../../ChicagoFaceDataset/CFD Version 2.0/CFD 2.0 Images/'\n",
    "    ext = 'N.jpg'\n",
    "else:\n",
    "    imPath = '../../MIT2kFaceDataset/2kfaces/'\n",
    "    ext = '.jpg'\n",
    "geometric_matrix = pd.read_csv('../../MIT2kFaceDataset/clean_data/geometric_all.csv',index_col = 0)\n",
    "imList = geometric_matrix.imgName.tolist()\n",
    "imList = [imPath+ imgStr for imgStr in imList]\n",
    "print len(imList)\n",
    "print imPath\n",
    "\n",
    "# Make sure that caffe is on the python path:\n",
    "caffe_root = '/home/lli-ms/caffe/'\n",
    "pretrained_model_root = '/home/lli-ms/caffe/'\n",
    "\n",
    "# run this line one time only!\n",
    "import sys\n",
    "caffePython = pretrained_model_root + 'python'\n",
    "if caffePython not in sys.path:\n",
    "    sys.path.insert(0, caffePython)\n",
    "\n",
    "\n",
    "import caffe\n",
    "# Load mean\n",
    "mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "print 'mean-subtracted values:', zip('BGR', mu)\n",
    "\n",
    "# Load the trained net\n",
    "MODEL = 'caffeNet' #'caffeNet','vgg16','vggFace' or 'faceSNN'\n",
    "\n",
    "saveFigPath = '../Result/'+Dataset+'/'+MODEL\n",
    "if not os.path.exists(saveFigPath):\n",
    "    os.makedirs(saveFigPath)\n",
    "    \n",
    "if MODEL == 'vgg16':\n",
    "    MODEL_FILE = caffe_root +'models/VGG16/VGG_ILSVRC_16_layers_deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/VGG16/VGG_ILSVRC_16_layers.caffemodel'\n",
    "elif MODEL == 'caffeNet':\n",
    "    MODEL_FILE = caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\n",
    "elif MODEL == 'vggFace':\n",
    "    MODEL_FILE = caffe_root + 'models/VGGFACE/VGG_CNN_F_deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/VGGFACE/VGG_CNN_F.caffemodel'\n",
    "    MEAN_FILE = caffe_root + 'models/VGGFACE/VGG_mean.binaryproto'\n",
    "else:\n",
    "    MODEL = 'faceSNN'\n",
    "    MODEL_FILE = caffe_root +'models/sraonet/siamese_lecun_deploy.prototxt'\n",
    "    PRETRAINED_FILE = caffe_root + 'models/sraonet/snapshots/sraonet_lecun_gd_sub2_iter_100000.caffemodel'\n",
    "    \n",
    "caffe.set_device(1)\n",
    "caffe.set_mode_gpu()\n",
    "if not os.path.isfile(PRETRAINED_FILE):\n",
    "    print(\"No caffemodel!!!\")\n",
    "elif not os.path.isfile(MODEL_FILE):\n",
    "    print(\"No MODEL !!!\")\n",
    "else:\n",
    "    print \"Defining the net!\"\n",
    "    net = caffe.Net(MODEL_FILE,\n",
    "                PRETRAINED_FILE,\n",
    "                caffe.TEST)\n",
    "# input preprocessing: 'data' is the name of the input blob == net.inputs[0]\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "if MODEL != 'faceSNN':\n",
    "    # subtract the dataset-mean value in each channel\n",
    "    transformer.set_mean('data', mu)\n",
    "    print 'featureNum: ',net.params['fc7'][1].data.shape\n",
    "else:\n",
    "    print 'featureNum: ',net.params['fc6'][1].data.shape\n",
    "transformer.set_transpose('data', (2,0,1))\n",
    "# the reference model operates on images in [0,255] range instead of [0,1]\n",
    "transformer.set_raw_scale('data', 255) \n",
    "# the reference model has channels in BGR order instead of RGB\n",
    "transformer.set_channel_swap('data', (2,1,0))\n",
    "# read in image list \n",
    "def readFile(fName):\n",
    "    text_file = open(fName, \"r\")\n",
    "    lines = text_file.read().split('\\n')\n",
    "    text_file.close()\n",
    "    return lines\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "if MODEL == 'vgg16' or MODEL == 'vggFace':\n",
    "    featureNum = 4096\n",
    "    imgeReshape = [224,224]\n",
    "    featureLayer = 'fc7'\n",
    "elif MODEL == 'caffeNet':\n",
    "    featureNum = 4096\n",
    "    imgeReshape = [227,227]\n",
    "    featureLayer = 'fc7'\n",
    "else:\n",
    "    featureNum = 50\n",
    "    imgeReshape = [56,46]\n",
    "    featureLayer = 'fc6'\n",
    "\n",
    "if Dataset == 'twin':\n",
    "    features = np.zeros([4,len(imList)/4,featureNum])\n",
    "    perImNum = len(imList)/4\n",
    "    img_type_num = {}\n",
    "    img_type_index = {}\n",
    "    img_type_list = {}\n",
    "    type_index = 0\n",
    "else:\n",
    "    features = np.zeros([len(imList),featureNum])\n",
    "totalNum = 0\n",
    "\n",
    "# print len(imList)\n",
    "for img in imList:\n",
    "    imgName = os.path.basename(img)\n",
    "    if imgName.endswith(('.jpg','.png')):\n",
    "        input_image = caffe.io.load_image(img)\n",
    "        net.blobs['data'].reshape(1,3,imgeReshape[0],imgeReshape[1])\n",
    "        net.blobs['data'].data[...] = transformer.preprocess('data', input_image)\n",
    "        out = net.forward()\n",
    "        feat = net.blobs[featureLayer].data\n",
    "        if Dataset =='twin':\n",
    "            img_type = int(imgName[7:-4])/perImNum\n",
    "            img_index = int(imgName[7:-4])%perImNum\n",
    "            #print 'img_type:',img_type\n",
    "            if img_type in img_type_num.keys():\n",
    "                img_type_num[img_type] = img_type_num[img_type] + 1\n",
    "                img_type_list[img_type][img_index] = img\n",
    "            else:\n",
    "                img_type_num[img_type] = 0\n",
    "                img_type_list[img_type] = [None]*perImNum\n",
    "                img_type_index[img_type] = type_index\n",
    "                type_index +=1\n",
    "            #print 'img_type_index:',img_type_index[img_type]\n",
    "            features[img_type_index[img_type],img_type_num[img_type]] = feat.flatten()\n",
    "        else:\n",
    "            # need to be further revised!\n",
    "            features[totalNum] = feat.flatten()\n",
    "            #print features[totalNum]\n",
    "        totalNum +=1\n",
    "    else:\n",
    "        print img\n",
    "#print len(img_type_num)\n",
    "print totalNum\n",
    "#print img_type_list\n",
    "\n",
    "if Dataset == 'twin':\n",
    "    featureMat = np.zeros((totalNum,featureNum))\n",
    "    k = 0\n",
    "    for i in range(features.shape[0]):\n",
    "        for j in range(features[i].shape[0]):\n",
    "            if sum(features[i,j,:])!=0:\n",
    "                featureMat[k,:] = features[i,j,:]\n",
    "                k +=1\n",
    "else:\n",
    "    featureMat = features\n",
    "#print featureMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'nose_width', u'nose_length', u'lip_thickness', u'face_length', u'eye_height', u'eye_width', u'face_width_prom', u'face_width_mouth', u'forehead_length', u'distance_btw_pupils', u'dist_btw_pupils_top', u'dist_btw_pupils_lip', u'chin_length', u'length_cheek_to_chin', u'brow_to_hair', u'fWHR', u'face_shape', u'heartshapeness', u'nose_shape', u'lip_fullness', u'eye_shape', u'eye_size', u'upper_head_len', u'midface_len', u'chin_size', u'forehead_height', u'cheek_height', u'cheek_prominence', u'face_roundness', u'FA', u'CA', u'AV'], dtype='object')\n",
      "The number of PCs needed to retain 0.990 variance is 506.\n",
      "The number of PCs needed to retain 0.990 variance is 886.\n",
      "The number of PCs needed to retain 0.990 variance is 9.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "if MODEL != 'faceSNN':\n",
    "    explained_variance = 0.99\n",
    "    #explained_variance = 50\n",
    "else:\n",
    "    explained_variance = featureNum\n",
    "\n",
    "config_feature = geometric_matrix.loc[:,'nose_width':'AV']\n",
    "print config_feature.columns \n",
    "feature_combine = np.concatenate((config_feature,featureMat),axis = 1)\n",
    "sklearn_pca = sklearnPCA(n_components=explained_variance, whiten  = True)\n",
    "feature_combine_transf = sklearn_pca.fit_transform(feature_combine)\n",
    "print 'The number of PCs needed to retain %.3f variance is %d.' \\\n",
    "      % (explained_variance, feature_combine_transf.shape[1])\n",
    "    \n",
    "sklearn_pca = sklearnPCA(n_components=explained_variance, whiten  = True)\n",
    "feature_transf = sklearn_pca.fit_transform(featureMat)\n",
    "print 'The number of PCs needed to retain %.3f variance is %d.' \\\n",
    "      % (explained_variance, feature_transf.shape[1])\n",
    "\n",
    "\n",
    "sklearn_pca = sklearnPCA(n_components=explained_variance, whiten  = True)\n",
    "config_transf = sklearn_pca.fit_transform(config_feature)\n",
    "print 'The number of PCs needed to retain %.3f variance is %d.' \\\n",
    "      % (explained_variance, config_transf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207\n",
      "(2207, 506)\n",
      "mean rating:  4.93915489035\n"
     ]
    }
   ],
   "source": [
    "mean_rating = geometric_matrix.attractive.as_matrix()\n",
    "print len(mean_rating)\n",
    "mean_rating = map(float, mean_rating)\n",
    "mean_rating = np.array(mean_rating)\n",
    "import sys\n",
    "#print sys.path\n",
    "# local\n",
    "# PkgPath = '/Users/Olivialinlin/Documents/Github/attractiveness_datamining/linjieCode/code'\n",
    "# server\n",
    "PkgPath = '/home/lli-ms/attractiveness_datamining/linjieCode/code'\n",
    "\n",
    "if PkgPath not in sys.path:\n",
    "    sys.path.insert(0, PkgPath)\n",
    "#print sys.path\n",
    "from xVal_train_test import Train_Test\n",
    "import pandas as pd\n",
    "feature_transf = feature_combine_transf #np.concatenate((config_transf,feature_transf),axis = 1)\n",
    "print feature_transf.shape\n",
    "baseLine = mean_rating.mean()\n",
    "print 'mean rating: ', baseLine\n",
    "import sklearn\n",
    "predictionModel = sklearn.linear_model.RidgeCV(alphas=np.logspace(-3,2,num=20), fit_intercept=True)\n",
    "Train_Test(mean_rating, feature_transf,xVal = True, pModel = predictionModel,\\\n",
    "               numTrain = 100,savePath = '../Result/'+Dataset,MODEL= MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "ratingPath = '../Result/'+Dataset+'/meanRating.csv'\n",
    "mean_rating = pd.read_csv(ratingPath,index_col = 0).as_matrix()[:,0].tolist()\n",
    "mean_rating = map(float, mean_rating)\n",
    "mean_rating = np.array(mean_rating)\n",
    "\n",
    "baseLine = mean_rating.mean()\n",
    "print 'mean rating: ', baseLine\n",
    "# cross validation to determine the number of features\n",
    "from sklearn import cross_validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                       feature_transf, mean_rating, test_size=0.2, random_state=0)\n",
    "corrList = []\n",
    "varList = []\n",
    "mseList = []\n",
    "if MODEL != 'faceSNN':\n",
    "    numFeature = [40,50,60,65,70,75,80,90,100,120,150,200,250,300,350]\n",
    "else:\n",
    "    numFeature = [10,20,30,40,50]\n",
    "for numF in numFeature:\n",
    "    X_train_hat = X_train[:,:numF]\n",
    "#     print X_train_hat.shape\n",
    "#     print y_train.shape\n",
    "    X_test_hat = X_test[:,:numF]\n",
    "    # Do linear regression on feature_arr and mean_rating\n",
    "    regr = linear_model.Ridge(fit_intercept=True)\n",
    "    regr.fit(X_train_hat, y_train)\n",
    "    predicted_rating = regr.predict(X_test_hat)\n",
    "    #rectified_rating = np.around(predicted_rating, decimals=0)\n",
    "    # Calculate the mean square error\n",
    "    MSE = np.mean((predicted_rating - y_test) ** 2)\n",
    "    mseList.append(MSE)\n",
    "    \n",
    "    # Returns the coefficient of determination R^2 of the prediction.\n",
    "    '''\n",
    "    The coefficient R^2 is defined as (1 - u/v), \n",
    "    where u is the regression sum of squares ((y_true - y_pred) ** 2).sum() \n",
    "    and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). \n",
    "    Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n",
    "    A constant model that always predicts the expected value of y, \n",
    "    disregarding the input features, would get a R^2 score of 0.0.\n",
    "    '''\n",
    "    variance_score = regr.score(X_test_hat, y_test)\n",
    "    varList.append(variance_score)\n",
    "    \n",
    "    # Calculate the correlation between prediction and actual rating.\n",
    "    cor = np.corrcoef(predicted_rating, y_test)\n",
    "    corrList.append(cor[0,1])\n",
    "\n",
    "print 'Correlation: ', max(corrList)\n",
    "print 'num of features: ',numFeature[np.argmax(corrList)]\n",
    "print 'R^2 score: ',max(varList)\n",
    "print 'num of features: ',numFeature[np.argmax(varList)]\n",
    "print 'MSE: ',min(mseList)\n",
    "print 'num of features: ',numFeature[np.argmin(mseList)]\n",
    "optNumFea = numFeature[np.argmax(corrList)]\n",
    "X_train_hat = X_train[:,:optNumFea]\n",
    "X_test_hat = X_test[:,:optNumFea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do linear regression on feature_arr and mean_rating\n",
    "regr = linear_model.Ridge(fit_intercept=True)\n",
    "regr.fit(X_train_hat, y_train)\n",
    "predicted_rating = regr.predict(X_train_hat)\n",
    "#rectified_rating = np.around(predicted_rating, decimals=0)\n",
    "\n",
    "# The coefficients\n",
    "#print 'Coefficients: ', regr.coef_[0:10]\n",
    "print 'Intercept: ', regr.intercept_\n",
    "# Calculate the mean square error\n",
    "MSE = np.mean((predicted_rating - y_train) ** 2)\n",
    "print 'Residual sum of squares: %.2f' % MSE\n",
    "\n",
    "# Calculate how much variance is explained\n",
    "variance_score = regr.score(X_train_hat, y_train)\n",
    "print 'Variance score is: %.2f' % variance_score\n",
    "\n",
    "# Calculate the correlation between prediction and actual rating.\n",
    "cor = np.corrcoef(predicted_rating, y_train)\n",
    "print 'Correlation is: %.2f' %cor[0, 1]\n",
    "\n",
    "fName = saveFigPath+'/'+MODEL+'_training.txt'\n",
    "with open(fName,'w') as f:\n",
    "    f.write('Training Accuracy\\n')\n",
    "    f.write('Number of features: %d'%X_train_hat.shape[1] +'\\n')\n",
    "    f.write('Residual sum of squares: %.2f' %MSE+'\\n')\n",
    "    f.write('Variance score is: %.2f' %variance_score+'\\n')\n",
    "    f.write('Correlation between predicted ratings and actual ratings is: %.4f'\\\n",
    "            %cor[0,1]+'\\n')  \n",
    "# # Plot prediction vs actual rating.\n",
    "x = predicted_rating\n",
    "y = y_train\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, alpha=0.5)\n",
    "ax.set_xlim((0, 8))\n",
    "ax.set_ylim((0, 8))\n",
    "x0, x1 = ax.get_xlim()\n",
    "y0, y1 = ax.get_ylim()\n",
    "ax.set_aspect(abs(x1-x0)/abs(y1-y0))\n",
    "ax.grid(b=True, which='major', color='k', linestyle='--')\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "X_plot = np.linspace(ax.get_xlim()[0],ax.get_xlim()[1],100)\n",
    "plt.plot(X_plot, m*X_plot + b, '-r')\n",
    "plt.xlabel('Predicted Ratings',fontsize = 26)\n",
    "plt.ylabel('Actual Ratings',fontsize = 26)\n",
    "plt.title('Predicted VS Actual Ratings',fontsize = 26)\n",
    "plt.savefig(saveFigPath+'/'+MODEL+'_predVsActual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k-fold#\n",
    "from sklearn.cross_validation import KFold\n",
    "imgNum = feature_transf.shape[0]\n",
    "foldNum = 20\n",
    "kf = KFold(imgNum, n_folds=foldNum, shuffle=True)\n",
    "print(kf)\n",
    "\n",
    "corrList = []\n",
    "varList = []\n",
    "mseList = []\n",
    "feature_new = feature_transf[:,:optNumFea]\n",
    "for train_index, test_index in kf:\n",
    "    feature_train, feature_test = feature_new[train_index], feature_new[test_index]\n",
    "    rating_train, rating_test = mean_rating[train_index], mean_rating[test_index]\n",
    "    \n",
    "    # Do linear regression on feature_arr and mean_rating\n",
    "    regr = linear_model.LinearRegression(fit_intercept=True)\n",
    "    regr.fit(feature_train, rating_train)\n",
    "    predicted_rating = regr.predict(feature_test)\n",
    "\n",
    "    # Calculate the mean square error\n",
    "    MSE = np.mean((predicted_rating - rating_test) ** 2)\n",
    "    mseList.append(MSE)\n",
    "    \n",
    "    # Returns the coefficient of determination R^2 of the prediction.\n",
    "    variance_score = regr.score(feature_test, rating_test)\n",
    "    varList.append(variance_score)\n",
    "    \n",
    "    # Calculate the correlation between prediction and actual rating.\n",
    "    cor = np.corrcoef(predicted_rating, rating_test)\n",
    "    corrList.append(cor[0,1])\n",
    "    \n",
    "print 'Residual sum of squares: %.2f' % (sum(mseList)/foldNum)\n",
    "print 'Variance score is: %.2f' % (sum(varList)/foldNum)\n",
    "print 'Correlation between predicted ratings and actual ratings is: %.4f'%(sum(corrList)/foldNum)\n",
    "\n",
    "fName = saveFigPath+'/'+MODEL+'_kFold.txt'\n",
    "with open(fName,'w') as f:\n",
    "    f.write('Number of folds: %d' % foldNum +'\\n')\n",
    "    f.write('Residual sum of squares: %.2f' % (sum(mseList)/foldNum)+'\\n')\n",
    "    f.write('Variance score is: %.2f' % (sum(varList)/foldNum)+'\\n')\n",
    "    f.write('Correlation between predicted ratings and actual ratings is: %.4f'\\\n",
    "            %(sum(corrList)/foldNum)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "np.savetxt(saveFigPath+'/feature_pca_atrr.csv', feature_new, delimiter=',')\n",
    "#np.savetxt(saveFigPath+'/feature_atrr.csv',featureMat,delimiter = ',')\n",
    "if Dataset == 'twin':\n",
    "    img_index_name_map = dict()\n",
    "    for key in img_type_list.keys():\n",
    "        img_index_name_map[img_type_index[key]] = img_type_list[key]\n",
    "    with open('../Result/'+Dataset+'/attr_imgIndex_name.pickle', 'wb') as handle:\n",
    "        pickle.dump(img_index_name_map, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
